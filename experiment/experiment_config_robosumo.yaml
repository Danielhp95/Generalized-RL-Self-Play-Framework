experiment:
    experiment_id: 'RoboschoolSumoTest-1Runs-1000Episodes-16Actors-PPO-lr4'
    environment: 'RoboschoolSumo-v0'
    number_of_runs: 1
    checkpoint_at_iterations: [50,100,150,200,250,300,350,400,450,500,550,600,650,700,750,800,850,900,950,1000]
    benchmarking_episodes: 20
    self_play_training_schemes: ['fullhistoryselfplay','halfhistoryselfplay','lastquarterhistoryselfplay']
    algorithms: ['ppo']
    fixed_agents: []

agents:
    deepqlearning:
        learning_rate: 1.0e-3
        epsstart: 0.8
        epsend: 0.05
        epsdecay: 1.0e3
        double: False
        dueling: False
        use_cuda: False
        use_PER: False
        PER_alpha: 0.07
        min_memory: 5.0e1
        memoryCapacity: 25.0e3
        nbrTrainIteration: 32
        batch_size: 256
        gamma: 0.99
        tau: 1.0e-2
    
    ppo_h2048lr4:                    
        horizon: 2048
        nbr_actor: 16
        discount: 0.99
        use_gae: True
        use_cuda: True
        gae_tau: 0.95
        entropy_weight: 0.01
        gradient_clip: 5
        optimization_epochs: 15
        mini_batch_size: 1024
        ppo_ratio_clip: 0.2
        learning_rate: 3.0e-4
        adam_eps: 1.0e-5
                                
    ddpg_lr3:
        discount: 0.99
        tau: 1e-3
        use_cuda: True
        nbrTrainIteration: 1 
        action_scaler: 1.0 
        use_HER: False
        HER_k: 2
        HER_strategy: 'future'
        HER_use_singlegoal: False 
        use_PER: True 
        PER_alpha: 0.7 
        replay_capacity: 25.0e3
        min_capacity: 5.0e3
        batch_size: 32
        learning_rate: 3.0e-4
        nbr_actor: 8